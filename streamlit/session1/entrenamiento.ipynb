{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: 202316187 (sergom2). Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m202316187\u001b[0m (\u001b[33msergom2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sergi\\OneDrive\\Escritorio\\MBD\\Machine Learning 2\\Practicas_DeepLearning_2024\\streamlit\\session1\\wandb\\run-20240411_223903-nqh91erg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sergom2/ML2-img-class/runs/nqh91erg' target=\"_blank\">Modelo1Grande</a></strong> to <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sergom2/ML2-img-class/runs/nqh91erg' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class/runs/nqh91erg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\anaconda3\\envs\\ML2\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sergi\\anaconda3\\envs\\ML2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1, Lote 1, Pérdida del Lote: 2.7169811725616455\n",
      "Época 1, Lote 11, Pérdida del Lote: 1.1506268978118896\n",
      "Época 1, Lote 21, Pérdida del Lote: 0.81302410364151\n",
      "Tiempo estimado restante: 360.44 minutos\n",
      "Epoch 1/25, Train Loss: 1.3586, Train Accuracy: 0.6238, Val Loss: 0.5721, Val Accuracy: 0.7807\n",
      "Época 2, Lote 1, Pérdida del Lote: 0.5070626139640808\n",
      "Época 2, Lote 11, Pérdida del Lote: 0.48501741886138916\n",
      "Época 2, Lote 21, Pérdida del Lote: 0.4069483280181885\n",
      "Tiempo estimado restante: 319.35 minutos\n",
      "Epoch 2/25, Train Loss: 0.4782, Train Accuracy: 0.8533, Val Loss: 0.3470, Val Accuracy: 0.8853\n",
      "Época 3, Lote 1, Pérdida del Lote: 0.48300760984420776\n",
      "Época 3, Lote 11, Pérdida del Lote: 0.35647052526474\n",
      "Época 3, Lote 21, Pérdida del Lote: 0.2899104654788971\n",
      "Tiempo estimado restante: 338.78 minutos\n",
      "Epoch 3/25, Train Loss: 0.3480, Train Accuracy: 0.8918, Val Loss: 0.2514, Val Accuracy: 0.9240\n",
      "Época 4, Lote 1, Pérdida del Lote: 0.27938735485076904\n",
      "Época 4, Lote 11, Pérdida del Lote: 0.21712249517440796\n",
      "Época 4, Lote 21, Pérdida del Lote: 0.27272990345954895\n",
      "Tiempo estimado restante: 354.91 minutos\n",
      "Epoch 4/25, Train Loss: 0.3002, Train Accuracy: 0.9032, Val Loss: 0.2082, Val Accuracy: 0.9367\n",
      "Época 5, Lote 1, Pérdida del Lote: 0.29405367374420166\n",
      "Época 5, Lote 11, Pérdida del Lote: 0.1437237411737442\n",
      "Época 5, Lote 21, Pérdida del Lote: 0.22571827471256256\n",
      "Tiempo estimado restante: 297.06 minutos\n",
      "Epoch 5/25, Train Loss: 0.2512, Train Accuracy: 0.9199, Val Loss: 0.2129, Val Accuracy: 0.9207\n",
      "Época 6, Lote 1, Pérdida del Lote: 0.15114320814609528\n",
      "Época 6, Lote 11, Pérdida del Lote: 0.17446638643741608\n",
      "Época 6, Lote 21, Pérdida del Lote: 0.28193148970603943\n",
      "Tiempo estimado restante: 289.66 minutos\n",
      "Epoch 6/25, Train Loss: 0.2330, Train Accuracy: 0.9303, Val Loss: 0.2098, Val Accuracy: 0.9280\n",
      "Época 7, Lote 1, Pérdida del Lote: 0.2726421058177948\n",
      "Época 7, Lote 11, Pérdida del Lote: 0.3075422942638397\n",
      "Época 7, Lote 21, Pérdida del Lote: 0.16673152148723602\n",
      "Tiempo estimado restante: 286.02 minutos\n",
      "Epoch 7/25, Train Loss: 0.2058, Train Accuracy: 0.9347, Val Loss: 0.1781, Val Accuracy: 0.9373\n",
      "Época 8, Lote 1, Pérdida del Lote: 0.149361252784729\n",
      "Época 8, Lote 11, Pérdida del Lote: 0.24021044373512268\n",
      "Época 8, Lote 21, Pérdida del Lote: 0.21232393383979797\n",
      "Tiempo estimado restante: 252.38 minutos\n",
      "Epoch 8/25, Train Loss: 0.2140, Train Accuracy: 0.9293, Val Loss: 0.2205, Val Accuracy: 0.9320\n",
      "Época 9, Lote 1, Pérdida del Lote: 0.16860376298427582\n",
      "Época 9, Lote 11, Pérdida del Lote: 0.24322529137134552\n",
      "Época 9, Lote 21, Pérdida del Lote: 0.1042870357632637\n",
      "Tiempo estimado restante: 303.63 minutos\n",
      "Epoch 9/25, Train Loss: 0.1868, Train Accuracy: 0.9407, Val Loss: 0.1752, Val Accuracy: 0.9413\n",
      "Época 10, Lote 1, Pérdida del Lote: 0.14885275065898895\n",
      "Época 10, Lote 11, Pérdida del Lote: 0.24411118030548096\n",
      "Época 10, Lote 21, Pérdida del Lote: 0.121286541223526\n",
      "Tiempo estimado restante: 238.92 minutos\n",
      "Epoch 10/25, Train Loss: 0.1505, Train Accuracy: 0.9508, Val Loss: 0.1625, Val Accuracy: 0.9480\n",
      "Época 11, Lote 1, Pérdida del Lote: 0.18290674686431885\n",
      "Época 11, Lote 11, Pérdida del Lote: 0.12657275795936584\n",
      "Época 11, Lote 21, Pérdida del Lote: 0.12726940214633942\n",
      "Tiempo estimado restante: 195.89 minutos\n",
      "Epoch 11/25, Train Loss: 0.1504, Train Accuracy: 0.9528, Val Loss: 0.1574, Val Accuracy: 0.9500\n",
      "Época 12, Lote 1, Pérdida del Lote: 0.1322847157716751\n",
      "Época 12, Lote 11, Pérdida del Lote: 0.11187732219696045\n",
      "Época 12, Lote 21, Pérdida del Lote: 0.14897626638412476\n",
      "Tiempo estimado restante: 173.68 minutos\n",
      "Epoch 12/25, Train Loss: 0.1397, Train Accuracy: 0.9578, Val Loss: 0.1562, Val Accuracy: 0.9473\n",
      "Época 13, Lote 1, Pérdida del Lote: 0.08290024846792221\n",
      "Época 13, Lote 11, Pérdida del Lote: 0.1526971459388733\n",
      "Época 13, Lote 21, Pérdida del Lote: 0.07637721300125122\n",
      "Tiempo estimado restante: 158.44 minutos\n",
      "Epoch 13/25, Train Loss: 0.1300, Train Accuracy: 0.9581, Val Loss: 0.1609, Val Accuracy: 0.9433\n",
      "Época 14, Lote 1, Pérdida del Lote: 0.09905847907066345\n",
      "Época 14, Lote 11, Pérdida del Lote: 0.156068354845047\n",
      "Época 14, Lote 21, Pérdida del Lote: 0.09627949446439743\n",
      "Tiempo estimado restante: 146.18 minutos\n",
      "Epoch 14/25, Train Loss: 0.1323, Train Accuracy: 0.9588, Val Loss: 0.1510, Val Accuracy: 0.9487\n",
      "Época 15, Lote 1, Pérdida del Lote: 0.15209399163722992\n",
      "Época 15, Lote 11, Pérdida del Lote: 0.15253402292728424\n",
      "Época 15, Lote 21, Pérdida del Lote: 0.0822986587882042\n",
      "Tiempo estimado restante: 130.55 minutos\n",
      "Epoch 15/25, Train Loss: 0.1170, Train Accuracy: 0.9642, Val Loss: 0.1541, Val Accuracy: 0.9507\n",
      "Época 16, Lote 1, Pérdida del Lote: 0.16449280083179474\n",
      "Época 16, Lote 11, Pérdida del Lote: 0.16247917711734772\n",
      "Época 16, Lote 21, Pérdida del Lote: 0.10787283629179001\n",
      "Tiempo estimado restante: 117.59 minutos\n",
      "Epoch 16/25, Train Loss: 0.1178, Train Accuracy: 0.9625, Val Loss: 0.1514, Val Accuracy: 0.9493\n",
      "Época 17, Lote 1, Pérdida del Lote: 0.06714294105768204\n",
      "Época 17, Lote 11, Pérdida del Lote: 0.063756063580513\n",
      "Época 17, Lote 21, Pérdida del Lote: 0.15197546780109406\n",
      "Tiempo estimado restante: 103.93 minutos\n",
      "Epoch 17/25, Train Loss: 0.1107, Train Accuracy: 0.9672, Val Loss: 0.1499, Val Accuracy: 0.9520\n",
      "Época 18, Lote 1, Pérdida del Lote: 0.13099946081638336\n",
      "Época 18, Lote 11, Pérdida del Lote: 0.07086356729269028\n",
      "Época 18, Lote 21, Pérdida del Lote: 0.1443573534488678\n",
      "Tiempo estimado restante: 92.24 minutos\n",
      "Epoch 18/25, Train Loss: 0.1185, Train Accuracy: 0.9615, Val Loss: 0.1486, Val Accuracy: 0.9540\n",
      "Época 19, Lote 1, Pérdida del Lote: 0.15313880145549774\n",
      "Época 19, Lote 11, Pérdida del Lote: 0.13416267931461334\n",
      "Época 19, Lote 21, Pérdida del Lote: 0.1299576312303543\n",
      "Tiempo estimado restante: 77.50 minutos\n",
      "Epoch 19/25, Train Loss: 0.1030, Train Accuracy: 0.9662, Val Loss: 0.1517, Val Accuracy: 0.9507\n",
      "Época 20, Lote 1, Pérdida del Lote: 0.11149561405181885\n",
      "Época 20, Lote 11, Pérdida del Lote: 0.0919022336602211\n",
      "Época 20, Lote 21, Pérdida del Lote: 0.1614062786102295\n",
      "Tiempo estimado restante: 65.00 minutos\n",
      "Epoch 20/25, Train Loss: 0.1210, Train Accuracy: 0.9601, Val Loss: 0.1461, Val Accuracy: 0.9507\n",
      "Época 21, Lote 1, Pérdida del Lote: 0.06956885010004044\n",
      "Época 21, Lote 11, Pérdida del Lote: 0.11018308252096176\n",
      "Época 21, Lote 21, Pérdida del Lote: 0.13993164896965027\n",
      "Tiempo estimado restante: 51.94 minutos\n",
      "Epoch 21/25, Train Loss: 0.1357, Train Accuracy: 0.9568, Val Loss: 0.1469, Val Accuracy: 0.9567\n",
      "Época 22, Lote 1, Pérdida del Lote: 0.10429134219884872\n",
      "Época 22, Lote 11, Pérdida del Lote: 0.09301990270614624\n",
      "Época 22, Lote 21, Pérdida del Lote: 0.14118130505084991\n",
      "Tiempo estimado restante: 39.23 minutos\n",
      "Epoch 22/25, Train Loss: 0.1112, Train Accuracy: 0.9642, Val Loss: 0.1454, Val Accuracy: 0.9540\n",
      "Época 23, Lote 1, Pérdida del Lote: 0.0765380859375\n",
      "Época 23, Lote 11, Pérdida del Lote: 0.08702025562524796\n",
      "Época 23, Lote 21, Pérdida del Lote: 0.15576867759227753\n",
      "Tiempo estimado restante: 26.37 minutos\n",
      "Epoch 23/25, Train Loss: 0.1202, Train Accuracy: 0.9591, Val Loss: 0.1462, Val Accuracy: 0.9513\n",
      "Época 24, Lote 1, Pérdida del Lote: 0.08103656768798828\n",
      "Época 24, Lote 11, Pérdida del Lote: 0.16094526648521423\n",
      "Época 24, Lote 21, Pérdida del Lote: 0.10362467169761658\n",
      "Tiempo estimado restante: 12.83 minutos\n",
      "Epoch 24/25, Train Loss: 0.1301, Train Accuracy: 0.9575, Val Loss: 0.1461, Val Accuracy: 0.9527\n",
      "Época 25, Lote 1, Pérdida del Lote: 0.07739992439746857\n",
      "Época 25, Lote 11, Pérdida del Lote: 0.14410482347011566\n",
      "Época 25, Lote 21, Pérdida del Lote: 0.10111460834741592\n",
      "Tiempo estimado restante: 0.00 minutos\n",
      "Epoch 25/25, Train Loss: 0.1155, Train Accuracy: 0.9635, Val Loss: 0.1472, Val Accuracy: 0.9520\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "# Configuración inicial de wandb\n",
    "wandb.init(project=\"ML2-img-class\", name=\"Modelo1Grande\", config={\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"architecture\": \"resnet50\",\n",
    "    \"dataset\": \"Tu Dataset Local\",\n",
    "    \"unfreezed_layers\": 10,  # Número de capas a descongelar\n",
    "    \"epochs\": 25,\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# Transformaciones\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder('C:/Users/sergi/OneDrive/Escritorio/MBD/Machine Learning 2/Practicas_DeepLearning_2024/03TransferLearning/dataset/training', transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder('C:/Users/sergi/OneDrive/Escritorio/MBD/Machine Learning 2/Practicas_DeepLearning_2024/03TransferLearning/dataset/validation', transform=valid_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=128, shuffle=False)\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Descongelar las últimas 10 capas\n",
    "children = list(model.children())\n",
    "num_children = len(children)\n",
    "for i in range(num_children - 10, num_children):\n",
    "    for param in children[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_data.classes))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = labels.size(0)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        \n",
    "        start_time_epoch = time.time()  # Tiempo de inicio de la época\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "            \n",
    "            if i % 10 == 0:  # Imprime cada 10 lotes\n",
    "                print(f'Época {epoch+1}, Lote {i+1}, Pérdida del Lote: {loss.item()}')\n",
    "        \n",
    "        end_time_epoch = time.time()  # Tiempo al final de la época\n",
    "        epoch_duration = end_time_epoch - start_time_epoch\n",
    "        estimated_time_left = epoch_duration * (num_epochs - epoch - 1)\n",
    "        print(f\"Tiempo estimado restante: {estimated_time_left/60:.2f} minutos\")\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = running_accuracy / len(train_loader.dataset)\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_accuracy = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "                \n",
    "        val_epoch_loss = val_running_loss / len(valid_loader.dataset)\n",
    "        val_epoch_accuracy = val_running_accuracy / len(valid_loader.dataset)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log a W&B\n",
    "        wandb.log({'train_loss': epoch_loss, 'train_accuracy': epoch_accuracy,\n",
    "                   'val_loss': val_epoch_loss, 'val_accuracy': val_epoch_accuracy})\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}, Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_epoch_accuracy:.4f}')\n",
    "               \n",
    "    return model\n",
    "\n",
    "model = train_model(model, criterion, optimizer, scheduler, num_epochs=25)\n",
    "torch.save(model.state_dict(), 'model_1_grande.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7dkih0gv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rural-darkness-12</strong> at: <a href='https://wandb.ai/sergom2/ML2-img-class/runs/7dkih0gv' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class/runs/7dkih0gv</a><br/> View project at: <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240412_083046-7dkih0gv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7dkih0gv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sergi\\OneDrive\\Escritorio\\MBD\\Machine Learning 2\\Practicas_DeepLearning_2024\\streamlit\\session1\\wandb\\run-20240412_083557-gtoi35ka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sergom2/ML2-img-class/runs/gtoi35ka' target=\"_blank\">densenet121_med</a></strong> to <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sergom2/ML2-img-class/runs/gtoi35ka' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class/runs/gtoi35ka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\anaconda3\\envs\\ML2\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sergi\\anaconda3\\envs\\ML2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Batch Loss: 2.9709556102752686\n",
      "Epoch 1, Batch 11, Batch Loss: 2.269395112991333\n",
      "Epoch 1, Batch 21, Batch Loss: 2.030111789703369\n",
      "Epoch 1, Batch 31, Batch Loss: 1.823324203491211\n",
      "Epoch 1, Batch 41, Batch Loss: 1.543107271194458\n",
      "Estimated time remaining: 47.68 minutes\n",
      "Epoch 2, Batch 1, Batch Loss: 1.2467236518859863\n",
      "Epoch 2, Batch 11, Batch Loss: 1.1343291997909546\n",
      "Epoch 2, Batch 21, Batch Loss: 1.1605753898620605\n",
      "Epoch 2, Batch 31, Batch Loss: 0.9497525691986084\n",
      "Epoch 2, Batch 41, Batch Loss: 0.8718527555465698\n",
      "Estimated time remaining: 44.69 minutes\n",
      "Epoch 3, Batch 1, Batch Loss: 0.9105361104011536\n",
      "Epoch 3, Batch 11, Batch Loss: 0.9864692091941833\n",
      "Epoch 3, Batch 21, Batch Loss: 0.725793719291687\n",
      "Epoch 3, Batch 31, Batch Loss: 0.829488217830658\n",
      "Epoch 3, Batch 41, Batch Loss: 0.7449735999107361\n",
      "Estimated time remaining: 39.54 minutes\n",
      "Epoch 4, Batch 1, Batch Loss: 0.6305000185966492\n",
      "Epoch 4, Batch 11, Batch Loss: 0.7752729654312134\n",
      "Epoch 4, Batch 21, Batch Loss: 0.9353387951850891\n",
      "Epoch 4, Batch 31, Batch Loss: 0.7590657472610474\n",
      "Epoch 4, Batch 41, Batch Loss: 0.6957582831382751\n",
      "Estimated time remaining: 34.02 minutes\n",
      "Epoch 5, Batch 1, Batch Loss: 0.798638105392456\n",
      "Epoch 5, Batch 11, Batch Loss: 0.7338468432426453\n",
      "Epoch 5, Batch 21, Batch Loss: 0.6326907277107239\n",
      "Epoch 5, Batch 31, Batch Loss: 0.5939747095108032\n",
      "Epoch 5, Batch 41, Batch Loss: 0.5866749882698059\n",
      "Estimated time remaining: 28.39 minutes\n",
      "Epoch 6, Batch 1, Batch Loss: 0.714771032333374\n",
      "Epoch 6, Batch 11, Batch Loss: 0.604010283946991\n",
      "Epoch 6, Batch 21, Batch Loss: 0.6547396779060364\n",
      "Epoch 6, Batch 31, Batch Loss: 0.5412963032722473\n",
      "Epoch 6, Batch 41, Batch Loss: 0.7169461250305176\n",
      "Estimated time remaining: 23.16 minutes\n",
      "Epoch 7, Batch 1, Batch Loss: 0.585651695728302\n",
      "Epoch 7, Batch 11, Batch Loss: 0.5075632333755493\n",
      "Epoch 7, Batch 21, Batch Loss: 0.58775794506073\n",
      "Epoch 7, Batch 31, Batch Loss: 0.791517972946167\n",
      "Epoch 7, Batch 41, Batch Loss: 0.6392093896865845\n",
      "Estimated time remaining: 16.93 minutes\n",
      "Epoch 8, Batch 1, Batch Loss: 0.6496090888977051\n",
      "Epoch 8, Batch 11, Batch Loss: 0.6905969977378845\n",
      "Epoch 8, Batch 21, Batch Loss: 0.5136740207672119\n",
      "Epoch 8, Batch 31, Batch Loss: 0.5644088983535767\n",
      "Epoch 8, Batch 41, Batch Loss: 0.6211428046226501\n",
      "Estimated time remaining: 11.71 minutes\n",
      "Epoch 9, Batch 1, Batch Loss: 0.563178300857544\n",
      "Epoch 9, Batch 11, Batch Loss: 0.695798933506012\n",
      "Epoch 9, Batch 21, Batch Loss: 0.45005685091018677\n",
      "Epoch 9, Batch 31, Batch Loss: 0.6054749488830566\n",
      "Epoch 9, Batch 41, Batch Loss: 0.5554506182670593\n",
      "Estimated time remaining: 5.77 minutes\n",
      "Epoch 10, Batch 1, Batch Loss: 0.7066293954849243\n",
      "Epoch 10, Batch 11, Batch Loss: 0.5987909436225891\n",
      "Epoch 10, Batch 21, Batch Loss: 0.7927845120429993\n",
      "Epoch 10, Batch 31, Batch Loss: 0.5262448191642761\n",
      "Epoch 10, Batch 41, Batch Loss: 0.5835970044136047\n",
      "Estimated time remaining: 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "# Configuración de Weights and Biases\n",
    "wandb.init(project=\"ML2-img-class\", name=\"densenet121_med\", config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"architecture\": \"densenet121\"\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# Transformaciones de datos\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Carga de datos\n",
    "train_data = datasets.ImageFolder('C:/Users/sergi/OneDrive/Escritorio/MBD/Machine Learning 2/Practicas_DeepLearning_2024/03TransferLearning/dataset/training', transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder('C:/Users/sergi/OneDrive/Escritorio/MBD/Machine Learning 2/Practicas_DeepLearning_2024/03TransferLearning/dataset/validation', transform=valid_transforms)\n",
    "train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Modelo\n",
    "model = models.densenet121(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Linear(num_ftrs, len(train_data.classes))\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_accuracy = 0, 0\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {i+1}, Batch Loss: {loss.item()}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = total_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = total_accuracy / len(train_loader.dataset)\n",
    "        val_loss, val_accuracy = evaluate(model, valid_loader, criterion)\n",
    "        wandb.log({'train_loss': epoch_loss, 'train_accuracy': epoch_accuracy, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "\n",
    "        end_time_epoch = time.time()\n",
    "        epoch_duration = end_time_epoch - start_time_epoch\n",
    "        estimated_time_left = epoch_duration * (num_epochs - epoch - 1)\n",
    "        print(f'Estimated time remaining: {estimated_time_left/60:.2f} minutes')\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_accuracy += (predicted == labels).sum().item()\n",
    "    return total_loss / len(loader.dataset), total_accuracy / len(loader.dataset)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "trained_model = train_model(model, criterion, optimizer, scheduler, num_epochs=config.epochs)\n",
    "torch.save(trained_model.state_dict(), 'densenet121_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gtoi35ka) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▇▇███████</td></tr><tr><td>train_loss</td><td>█▃▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▇▇██████</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.82982</td></tr><tr><td>train_loss</td><td>0.63629</td></tr><tr><td>val_accuracy</td><td>0.872</td></tr><tr><td>val_loss</td><td>0.47158</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">densenet121_med</strong> at: <a href='https://wandb.ai/sergom2/ML2-img-class/runs/gtoi35ka' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class/runs/gtoi35ka</a><br/> View project at: <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240412_083557-gtoi35ka\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gtoi35ka). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sergi\\OneDrive\\Escritorio\\MBD\\Machine Learning 2\\Practicas_DeepLearning_2024\\streamlit\\session1\\wandb\\run-20240412_093250-lewl69kb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sergom2/ML2-img-class/runs/lewl69kb' target=\"_blank\">efficientnet_b0_med</a></strong> to <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sergom2/ML2-img-class/runs/lewl69kb' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class/runs/lewl69kb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\anaconda3\\envs\\ML2\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sergi\\anaconda3\\envs\\ML2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to C:\\Users\\sergi/.cache\\torch\\hub\\checkpoints\\efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 22.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Batch Loss: 2.7310023307800293\n",
      "Epoch 1, Batch 11, Batch Loss: 2.4098026752471924\n",
      "Epoch 1, Batch 21, Batch Loss: 2.118462324142456\n",
      "Epoch 1, Batch 31, Batch Loss: 1.7233350276947021\n",
      "Epoch 1, Batch 41, Batch Loss: 1.486593246459961\n",
      "Estimated time remaining: 30.03 minutes\n",
      "Epoch 2, Batch 1, Batch Loss: 1.3617490530014038\n",
      "Epoch 2, Batch 11, Batch Loss: 1.1080161333084106\n",
      "Epoch 2, Batch 21, Batch Loss: 1.1564067602157593\n",
      "Epoch 2, Batch 31, Batch Loss: 1.1161137819290161\n",
      "Epoch 2, Batch 41, Batch Loss: 1.0970815420150757\n",
      "Estimated time remaining: 24.88 minutes\n",
      "Epoch 3, Batch 1, Batch Loss: 1.1628406047821045\n",
      "Epoch 3, Batch 11, Batch Loss: 1.0470322370529175\n",
      "Epoch 3, Batch 21, Batch Loss: 0.9121251702308655\n",
      "Epoch 3, Batch 31, Batch Loss: 0.9395942687988281\n",
      "Epoch 3, Batch 41, Batch Loss: 0.9646850228309631\n",
      "Estimated time remaining: 23.58 minutes\n",
      "Epoch 4, Batch 1, Batch Loss: 0.7534129619598389\n",
      "Epoch 4, Batch 11, Batch Loss: 0.9177544116973877\n",
      "Epoch 4, Batch 21, Batch Loss: 0.7910711765289307\n",
      "Epoch 4, Batch 31, Batch Loss: 0.644273042678833\n",
      "Epoch 4, Batch 41, Batch Loss: 0.7015964984893799\n",
      "Estimated time remaining: 19.31 minutes\n",
      "Epoch 5, Batch 1, Batch Loss: 0.7705138921737671\n",
      "Epoch 5, Batch 11, Batch Loss: 0.6355867981910706\n",
      "Epoch 5, Batch 21, Batch Loss: 0.7777313590049744\n",
      "Epoch 5, Batch 31, Batch Loss: 0.7485350370407104\n",
      "Epoch 5, Batch 41, Batch Loss: 0.6987003087997437\n",
      "Estimated time remaining: 17.03 minutes\n",
      "Epoch 6, Batch 1, Batch Loss: 0.7356797456741333\n",
      "Epoch 6, Batch 11, Batch Loss: 0.6707680821418762\n",
      "Epoch 6, Batch 21, Batch Loss: 0.8957303166389465\n",
      "Epoch 6, Batch 31, Batch Loss: 0.6333180665969849\n",
      "Epoch 6, Batch 41, Batch Loss: 0.9973501563072205\n",
      "Estimated time remaining: 12.67 minutes\n",
      "Epoch 7, Batch 1, Batch Loss: 0.6600887775421143\n",
      "Epoch 7, Batch 11, Batch Loss: 0.7958829998970032\n",
      "Epoch 7, Batch 21, Batch Loss: 0.7931455373764038\n",
      "Epoch 7, Batch 31, Batch Loss: 0.9006316065788269\n",
      "Epoch 7, Batch 41, Batch Loss: 0.7633089423179626\n",
      "Estimated time remaining: 9.92 minutes\n",
      "Epoch 8, Batch 1, Batch Loss: 0.7905885577201843\n",
      "Epoch 8, Batch 11, Batch Loss: 0.7654919624328613\n",
      "Epoch 8, Batch 21, Batch Loss: 0.7566022276878357\n",
      "Epoch 8, Batch 31, Batch Loss: 0.7653461694717407\n",
      "Epoch 8, Batch 41, Batch Loss: 0.636417806148529\n",
      "Estimated time remaining: 6.42 minutes\n",
      "Epoch 9, Batch 1, Batch Loss: 0.6551854014396667\n",
      "Epoch 9, Batch 11, Batch Loss: 0.7353819608688354\n",
      "Epoch 9, Batch 21, Batch Loss: 0.8188343048095703\n",
      "Epoch 9, Batch 31, Batch Loss: 0.8048310279846191\n",
      "Epoch 9, Batch 41, Batch Loss: 0.7004880905151367\n",
      "Estimated time remaining: 3.16 minutes\n",
      "Epoch 10, Batch 1, Batch Loss: 0.6069802641868591\n",
      "Epoch 10, Batch 11, Batch Loss: 0.7993770241737366\n",
      "Epoch 10, Batch 21, Batch Loss: 0.6301311254501343\n",
      "Epoch 10, Batch 31, Batch Loss: 0.8091980814933777\n",
      "Epoch 10, Batch 41, Batch Loss: 0.7513743042945862\n",
      "Estimated time remaining: 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "# Configuración de Weights and Biases\n",
    "wandb.init(project=\"ML2-img-class\", name=\"efficientnet_b0_med\",config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"architecture\": \"efficientnet_b0\"\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# Transformaciones de datos\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Carga de datos\n",
    "train_data = datasets.ImageFolder('C:/Users/sergi/OneDrive/Escritorio/MBD/Machine Learning 2/Practicas_DeepLearning_2024/03TransferLearning/dataset/training', transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder('C:/Users/sergi/OneDrive/Escritorio/MBD/Machine Learning 2/Practicas_DeepLearning_2024/03TransferLearning/dataset/validation', transform=valid_transforms)\n",
    "train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Modelo\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, len(train_data.classes))\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_accuracy = 0, 0\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {i+1}, Batch Loss: {loss.item()}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = total_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = total_accuracy / len(train_loader.dataset)\n",
    "        val_loss, val_accuracy = evaluate(model, valid_loader, criterion)\n",
    "        wandb.log({'train_loss': epoch_loss, 'train_accuracy': epoch_accuracy, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "\n",
    "        end_time_epoch = time.time()\n",
    "        epoch_duration = end_time_epoch - start_time_epoch\n",
    "        estimated_time_left = epoch_duration * (num_epochs - epoch - 1)\n",
    "        print(f'Estimated time remaining: {estimated_time_left/60:.2f} minutes')\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_accuracy += (predicted == labels).sum().item()\n",
    "    return total_loss / len(loader.dataset), total_accuracy / len(loader.dataset)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "trained_model = train_model(model, criterion, optimizer, scheduler, num_epochs=config.epochs)\n",
    "torch.save(trained_model.state_dict(), 'efficientnet_b0_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lewl69kb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▆▇███████</td></tr><tr><td>train_loss</td><td>█▄▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.79464</td></tr><tr><td>train_loss</td><td>0.74009</td></tr><tr><td>val_accuracy</td><td>0.82933</td></tr><tr><td>val_loss</td><td>0.63816</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficientnet_b0_med</strong> at: <a href='https://wandb.ai/sergom2/ML2-img-class/runs/lewl69kb' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class/runs/lewl69kb</a><br/> View project at: <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240412_093250-lewl69kb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lewl69kb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sergi\\OneDrive\\Escritorio\\MBD\\Machine Learning 2\\Practicas_DeepLearning_2024\\streamlit\\session1\\wandb\\run-20240412_100604-3us44pb2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sergom2/ML2-img-class/runs/3us44pb2' target=\"_blank\">mobilnetv2_med</a></strong> to <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sergom2/ML2-img-class/runs/3us44pb2' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class/runs/3us44pb2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\anaconda3\\envs\\ML2\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sergi\\anaconda3\\envs\\ML2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\sergi/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n",
      "100%|██████████| 13.6M/13.6M [00:00<00:00, 42.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Batch Loss: 2.821324110031128\n",
      "Epoch 1, Batch 11, Batch Loss: 2.1060397624969482\n",
      "Epoch 1, Batch 21, Batch Loss: 1.6546361446380615\n",
      "Epoch 1, Batch 31, Batch Loss: 1.1582417488098145\n",
      "Epoch 1, Batch 41, Batch Loss: 1.2890992164611816\n",
      "Estimated time remaining: 17.38 minutes\n",
      "Epoch 2, Batch 1, Batch Loss: 0.997840940952301\n",
      "Epoch 2, Batch 11, Batch Loss: 0.887863278388977\n",
      "Epoch 2, Batch 21, Batch Loss: 0.8057731986045837\n",
      "Epoch 2, Batch 31, Batch Loss: 0.8361971974372864\n",
      "Epoch 2, Batch 41, Batch Loss: 0.6521124839782715\n",
      "Estimated time remaining: 14.98 minutes\n",
      "Epoch 3, Batch 1, Batch Loss: 0.6037158966064453\n",
      "Epoch 3, Batch 11, Batch Loss: 0.6404091119766235\n",
      "Epoch 3, Batch 21, Batch Loss: 0.6929765343666077\n",
      "Epoch 3, Batch 31, Batch Loss: 0.8236346244812012\n",
      "Epoch 3, Batch 41, Batch Loss: 0.651573896408081\n",
      "Estimated time remaining: 13.36 minutes\n",
      "Epoch 4, Batch 1, Batch Loss: 0.5225253105163574\n",
      "Epoch 4, Batch 11, Batch Loss: 0.6342203617095947\n",
      "Epoch 4, Batch 21, Batch Loss: 0.6989042162895203\n",
      "Epoch 4, Batch 31, Batch Loss: 0.7207708358764648\n",
      "Epoch 4, Batch 41, Batch Loss: 0.5712652206420898\n",
      "Estimated time remaining: 11.37 minutes\n",
      "Epoch 5, Batch 1, Batch Loss: 0.5519234538078308\n",
      "Epoch 5, Batch 11, Batch Loss: 0.7362905144691467\n",
      "Epoch 5, Batch 21, Batch Loss: 0.4620858430862427\n",
      "Epoch 5, Batch 31, Batch Loss: 0.46783262491226196\n",
      "Epoch 5, Batch 41, Batch Loss: 0.7048544883728027\n",
      "Estimated time remaining: 9.81 minutes\n",
      "Epoch 6, Batch 1, Batch Loss: 0.5765978097915649\n",
      "Epoch 6, Batch 11, Batch Loss: 0.5529913902282715\n",
      "Epoch 6, Batch 21, Batch Loss: 0.6266359090805054\n",
      "Epoch 6, Batch 31, Batch Loss: 0.5669716596603394\n",
      "Epoch 6, Batch 41, Batch Loss: 0.5746716856956482\n",
      "Estimated time remaining: 7.66 minutes\n",
      "Epoch 7, Batch 1, Batch Loss: 0.43309593200683594\n",
      "Epoch 7, Batch 11, Batch Loss: 0.4256048798561096\n",
      "Epoch 7, Batch 21, Batch Loss: 0.5862309336662292\n",
      "Epoch 7, Batch 31, Batch Loss: 0.604780375957489\n",
      "Epoch 7, Batch 41, Batch Loss: 0.5363593101501465\n",
      "Estimated time remaining: 5.74 minutes\n",
      "Epoch 8, Batch 1, Batch Loss: 0.518243134021759\n",
      "Epoch 8, Batch 11, Batch Loss: 0.42380309104919434\n",
      "Epoch 8, Batch 21, Batch Loss: 0.4677938222885132\n",
      "Epoch 8, Batch 31, Batch Loss: 0.560970664024353\n",
      "Epoch 8, Batch 41, Batch Loss: 0.5551071763038635\n",
      "Estimated time remaining: 3.84 minutes\n",
      "Epoch 9, Batch 1, Batch Loss: 0.5636729598045349\n",
      "Epoch 9, Batch 11, Batch Loss: 0.56548011302948\n",
      "Epoch 9, Batch 21, Batch Loss: 0.44021761417388916\n",
      "Epoch 9, Batch 31, Batch Loss: 0.6284316182136536\n",
      "Epoch 9, Batch 41, Batch Loss: 0.6892155408859253\n",
      "Estimated time remaining: 1.94 minutes\n",
      "Epoch 10, Batch 1, Batch Loss: 0.555817186832428\n",
      "Epoch 10, Batch 11, Batch Loss: 0.6910953521728516\n",
      "Epoch 10, Batch 21, Batch Loss: 0.36993876099586487\n",
      "Epoch 10, Batch 31, Batch Loss: 0.5032564997673035\n",
      "Epoch 10, Batch 41, Batch Loss: 0.3827130198478699\n",
      "Estimated time remaining: 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "# Configuración de Weights and Biases\n",
    "wandb.init(project=\"ML2-img-class\", name=\"mobilnetv2_med\",config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"architecture\": \"mobilenet_v2\"\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# Transformaciones de datos\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Carga de datos\n",
    "train_data = datasets.ImageFolder('C:/Users/sergi/OneDrive/Escritorio/MBD/Machine Learning 2/Practicas_DeepLearning_2024/03TransferLearning/dataset/training', transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder('C:/Users/sergi/OneDrive/Escritorio/MBD/Machine Learning 2/Practicas_DeepLearning_2024/03TransferLearning/dataset/validation', transform=valid_transforms)\n",
    "train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Modelo\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[1] = nn.Linear(model.last_channel, len(train_data.classes))\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_accuracy = 0, 0\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {i+1}, Batch Loss: {loss.item()}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = total_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = total_accuracy / len(train_loader.dataset)\n",
    "        val_loss, val_accuracy = evaluate(model, valid_loader, criterion)\n",
    "        wandb.log({'train_loss': epoch_loss, 'train_accuracy': epoch_accuracy, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "\n",
    "        end_time_epoch = time.time()\n",
    "        epoch_duration = end_time_epoch - start_time_epoch\n",
    "        estimated_time_left = epoch_duration * (num_epochs - epoch - 1)\n",
    "        print(f'Estimated time remaining: {estimated_time_left/60:.2f} minutes')\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_accuracy += (predicted == labels).sum().item()\n",
    "    return total_loss / len(loader.dataset), total_accuracy / len(loader.dataset)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "trained_model = train_model(model, criterion, optimizer, scheduler, num_epochs=config.epochs)\n",
    "torch.save(trained_model.state_dict(), 'mobilenet_v2_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3us44pb2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▇▆▇▇███▇</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.8392</td></tr><tr><td>train_loss</td><td>0.52859</td></tr><tr><td>val_accuracy</td><td>0.87733</td></tr><tr><td>val_loss</td><td>0.37016</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mobilnetv2_med</strong> at: <a href='https://wandb.ai/sergom2/ML2-img-class/runs/3us44pb2' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class/runs/3us44pb2</a><br/> View project at: <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240412_100604-3us44pb2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3us44pb2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sergi\\OneDrive\\Escritorio\\MBD\\Machine Learning 2\\Practicas_DeepLearning_2024\\streamlit\\session1\\wandb\\run-20240412_120723-g6zoav2j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sergom2/ML2-img-class/runs/g6zoav2j' target=\"_blank\">shufflenet_v2_med</a></strong> to <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sergom2/ML2-img-class' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sergom2/ML2-img-class/runs/g6zoav2j' target=\"_blank\">https://wandb.ai/sergom2/ML2-img-class/runs/g6zoav2j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\anaconda3\\envs\\ML2\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sergi\\anaconda3\\envs\\ML2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\" to C:\\Users\\sergi/.cache\\torch\\hub\\checkpoints\\shufflenetv2_x1-5666bf0f80.pth\n",
      "100%|██████████| 8.79M/8.79M [00:00<00:00, 38.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Batch Loss: 2.706932783126831\n",
      "Epoch 1, Batch 11, Batch Loss: 2.6652212142944336\n",
      "Epoch 1, Batch 21, Batch Loss: 2.629255533218384\n",
      "Epoch 1, Batch 31, Batch Loss: 2.59684681892395\n",
      "Epoch 1, Batch 41, Batch Loss: 2.562190532684326\n",
      "Estimated time remaining: 9.54 minutes\n",
      "Epoch 2, Batch 1, Batch Loss: 2.5428006649017334\n",
      "Epoch 2, Batch 11, Batch Loss: 2.524078607559204\n",
      "Epoch 2, Batch 21, Batch Loss: 2.4842708110809326\n",
      "Epoch 2, Batch 31, Batch Loss: 2.475432872772217\n",
      "Epoch 2, Batch 41, Batch Loss: 2.47778058052063\n",
      "Estimated time remaining: 5.93 minutes\n",
      "Epoch 3, Batch 1, Batch Loss: 2.4666359424591064\n",
      "Epoch 3, Batch 11, Batch Loss: 2.4213998317718506\n",
      "Epoch 3, Batch 21, Batch Loss: 2.386531114578247\n",
      "Epoch 3, Batch 31, Batch Loss: 2.3473312854766846\n",
      "Epoch 3, Batch 41, Batch Loss: 2.40496563911438\n",
      "Estimated time remaining: 5.34 minutes\n",
      "Epoch 4, Batch 1, Batch Loss: 2.310044050216675\n",
      "Epoch 4, Batch 11, Batch Loss: 2.371171236038208\n",
      "Epoch 4, Batch 21, Batch Loss: 2.2973647117614746\n",
      "Epoch 4, Batch 31, Batch Loss: 2.327294111251831\n",
      "Epoch 4, Batch 41, Batch Loss: 2.2829442024230957\n",
      "Estimated time remaining: 5.49 minutes\n",
      "Epoch 5, Batch 1, Batch Loss: 2.3137941360473633\n",
      "Epoch 5, Batch 11, Batch Loss: 2.217460870742798\n",
      "Epoch 5, Batch 21, Batch Loss: 2.296811580657959\n",
      "Epoch 5, Batch 31, Batch Loss: 2.273158311843872\n",
      "Epoch 5, Batch 41, Batch Loss: 2.2240540981292725\n",
      "Estimated time remaining: 3.88 minutes\n",
      "Epoch 6, Batch 1, Batch Loss: 2.208263635635376\n",
      "Epoch 6, Batch 11, Batch Loss: 2.292768716812134\n",
      "Epoch 6, Batch 21, Batch Loss: 2.241386890411377\n",
      "Epoch 6, Batch 31, Batch Loss: 2.1706955432891846\n",
      "Epoch 6, Batch 41, Batch Loss: 2.233181953430176\n",
      "Estimated time remaining: 3.03 minutes\n",
      "Epoch 7, Batch 1, Batch Loss: 2.174381971359253\n",
      "Epoch 7, Batch 11, Batch Loss: 2.259706497192383\n",
      "Epoch 7, Batch 21, Batch Loss: 2.2398922443389893\n",
      "Epoch 7, Batch 31, Batch Loss: 2.2603509426116943\n",
      "Epoch 7, Batch 41, Batch Loss: 2.1800553798675537\n",
      "Estimated time remaining: 2.55 minutes\n",
      "Epoch 8, Batch 1, Batch Loss: 2.1937901973724365\n",
      "Epoch 8, Batch 11, Batch Loss: 2.2387707233428955\n",
      "Epoch 8, Batch 21, Batch Loss: 2.2302722930908203\n",
      "Epoch 8, Batch 31, Batch Loss: 2.1608150005340576\n",
      "Epoch 8, Batch 41, Batch Loss: 2.1648707389831543\n",
      "Estimated time remaining: 1.91 minutes\n",
      "Epoch 9, Batch 1, Batch Loss: 2.2212328910827637\n",
      "Epoch 9, Batch 11, Batch Loss: 2.1474266052246094\n",
      "Epoch 9, Batch 21, Batch Loss: 2.313382863998413\n",
      "Epoch 9, Batch 31, Batch Loss: 2.1848666667938232\n",
      "Epoch 9, Batch 41, Batch Loss: 2.22377610206604\n",
      "Estimated time remaining: 0.76 minutes\n",
      "Epoch 10, Batch 1, Batch Loss: 2.2036845684051514\n",
      "Epoch 10, Batch 11, Batch Loss: 2.1743199825286865\n",
      "Epoch 10, Batch 21, Batch Loss: 2.279216766357422\n",
      "Epoch 10, Batch 31, Batch Loss: 2.1950881481170654\n",
      "Epoch 10, Batch 41, Batch Loss: 2.2116243839263916\n",
      "Estimated time remaining: 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "# Configuración de Weights and Biases\n",
    "wandb.init(project=\"ML2-img-class\", name=\"shufflenet_v2_med\",config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"architecture\": \"shufflenet_v2_x1_0\"\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# Transformaciones de datos\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Carga de datos\n",
    "train_data = datasets.ImageFolder('C:/Users/sergi/OneDrive/Escritorio/MBD/Machine Learning 2/Practicas_DeepLearning_2024/03TransferLearning/dataset/training', transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder('C:/Users/sergi/OneDrive/Escritorio/MBD/Machine Learning 2/Practicas_DeepLearning_2024/03TransferLearning/dataset/validation', transform=valid_transforms)\n",
    "train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Modelo\n",
    "model = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_data.classes))\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_accuracy = 0, 0\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {i+1}, Batch Loss: {loss.item()}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = total_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = total_accuracy / len(train_loader.dataset)\n",
    "        val_loss, val_accuracy = evaluate(model, valid_loader, criterion)\n",
    "        wandb.log({'train_loss': epoch_loss, 'train_accuracy': epoch_accuracy, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "\n",
    "        end_time_epoch = time.time()\n",
    "        epoch_duration = end_time_epoch - start_time_epoch\n",
    "        estimated_time_left = epoch_duration * (num_epochs - epoch - 1)\n",
    "        print(f'Estimated time remaining: {estimated_time_left/60:.2f} minutes')\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_accuracy += (predicted == labels).sum().item()\n",
    "    return total_loss / len(loader.dataset), total_accuracy / len(loader.dataset)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "trained_model = train_model(model, criterion, optimizer, scheduler, num_epochs=config.epochs)\n",
    "torch.save(trained_model.state_dict(), 'shufflenet_v2_x1_0_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
